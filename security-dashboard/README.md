# Security Dashboard - Enterprise Blueprint

## Overview
This folder now provides an enterprise-ready blueprint that unifies every other capability that ships with the `Cybersecurity-Portfolio` repository.  It exposes a single observability plane that can ingest telemetry generated by:

| Capability | Repository path | Primary data product | Dashboard index |
|------------|-----------------|----------------------|-----------------|
| Automated recon & exploit validation | `Security_Scripts/` | Findings from tools such as `port-scanner`, `vuln_check`, `password_spray`, and `shodan-scanner` | `security-scans-*` |
| Custom IDS and detection engineering | `custom-ids/` | Alerts created by the curated Suricata/Snort rulesets | `ids-alerts-*` |
| Secure configuration automation | `ansible-hardening/` | Compliance status per control and host | `ansible-compliance-*` |
| Tamper evident ledgering | `blockchain-secure-logging/` | Blockchain anchoring state, off-chain notarization metadata | `blockchain-audit-*` |
| Mobile application analysis | `mobile-security-analysis/` | Static and dynamic risk scoring for APKs | `mobile-findings-*` |
| Quantum/RNG experimentation | `quantum-computing/` | QRNG health, Kyber posture, and other emerging-tech experiments | `quantum-research-*` |

The upgraded stack combines Elasticsearch, Logstash, Kibana, and Grafana.  Logstash normalizes JSON artifacts produced by each toolset, Elasticsearch stores each domain inside its own time-series index, Kibana ships opinionated dashboards for SOC workflows, and Grafana provides SRE-friendly health boards.  Everything runs locally through Docker Compose so that the blueprint can be hardened and scaled within your preferred orchestrator afterwards.

## Stack components

| Service | Purpose |
|---------|---------|
| **Elasticsearch** | Schema-flexible datastore that backs the dashboard. Security monitoring indices are prefixed per capability so lifecycle and security policies can be tailored. |
| **Logstash** | File-based ingest pipeline that reads JSON Lines events from `data/ingest/`, enriches them with metadata, and publishes to the appropriate index. Beats, Kafka, or HTTP inputs can be enabled by extending `logstash/pipeline/logstash.conf`. |
| **Kibana** | Analyst UX. The blueprint includes index-pattern definitions, visualization descriptions, and a multi-panel dashboard skeleton in `kibana/`. Import them via Stack Management → Saved Objects. |
| **Grafana** | Provides a parallel, ops-friendly view that can mix Elasticsearch and external metrics. Datasource provisioning lives in `grafana/provisioning/`. |

## Repository-aware data model
Each major tool in the portfolio emits JSON documents with a `target_index` field.  Logstash reads the field and writes to an index of the same name.  The sample dataset stored in `data/ingest/enterprise-events.jsonl` demonstrates the mapping and references real artifacts from across the repo (for example `custom-ids/rules/enterprise.rules` and `Security_Scripts/vuln_check`).

The helper script `scripts/build_enterprise_dataset.py` regenerates the JSON Lines file and injects live metadata (for example, the number of IDS rule packs or the list of reconnaissance scripts that are present on disk).  Treat the script as the canonical contract for field names when wiring in additional tools.

## Quick start
1. **Generate / refresh sample data**
   ```bash
   python3 scripts/build_enterprise_dataset.py
   ```
   The script inspects the repository tree and rewrites `data/ingest/enterprise-events.jsonl` with synthetic yet realistic events for every capability in the table above.
2. **Launch the stack**
   ```bash
   docker-compose up -d
   ```
   The compose file now starts Elasticsearch, Logstash, Kibana, and Grafana.  Logstash immediately begins reading the JSON Lines file and publishing documents into the six security indices plus the `platform-inventory` summary index.
3. **Import the dashboard blueprints**
   * In Kibana, open *Stack Management → Saved Objects → Import* and feed it the files under `kibana/`.  They describe index patterns, visualization goals, and a SOC dashboard layout tailored to this repository.
   * In Grafana, log in with the default admin credentials (`admin` / `changeme`, change on first login).  The Elasticsearch datasource is provisioned automatically; add panels that query the indices defined above or reuse the JSON from `grafana/dashboards/`.
4. **Point real tools at the stack**
   * Drop JSON Lines outputs from scripts inside `Security_Scripts/` or reports emitted by `mobile-security-analysis/analyze_apk.py` into `data/ingest/` and rerun Logstash (or send them via Filebeat/HTTP).
   * Forward Suricata/Snort EVE JSON from `custom-ids/` sensors to the Beats port (`5044`) exposed by Logstash.
   * Stream compliance summaries generated by `ansible-hardening/` playbooks or blockchain receipts from `blockchain-secure-logging/` to the same ingest path.

## Extending the blueprint
- **Index lifecycle and security** – In production enable TLS and role-based access control, carve data streams per capability, and attach ILM policies based on retention requirements.
- **Automation** – Wire the scripts in `Security_Scripts/` and the playbooks under `ansible-hardening/` to CI/CD so that every run ships JSON to this dashboard automatically.
- **Alerting** – Enable Kibana alerting or Grafana OnCall using the provided visualizations as starting points.  For example, build threshold alerts on `ids-alerts-*` severity counts.
- **Ledger verification** – Use `blockchain-secure-logging/offchain/` services to verify hashes displayed on the blockchain audit panel before closing incidents.

## File map
```
security-dashboard/
├── data/ingest/enterprise-events.jsonl    # Repository-aware sample events
├── docker-compose.yml                    # Multi-service stack definition
├── logstash/pipeline/logstash.conf       # Metadata-aware ingest pipeline
├── grafana/provisioning/                 # Automatic Grafana datasource config
├── kibana/                               # Index patterns, visualization specs, and dashboard layout
├── scripts/build_enterprise_dataset.py   # Regenerate the JSON Lines dataset
└── scripts/architecture.md               # Detailed description of the enterprise topology
```

## License & disclaimer
This project is for educational and authorized professional use only. Do not use on unauthorized networks or systems.  Always obtain permission before running active security tooling.
