# Security Dashboard - Enterprise Blueprint

## Overview
This folder now provides an enterprise-ready blueprint that unifies every other capability that ships with the `Cybersecurity-Portfolio` repository.  It exposes a single observability plane that can ingest telemetry generated by:

| Capability | Repository path | Primary data product | Dashboard index |
|------------|-----------------|----------------------|-----------------|
| Automated recon & exploit validation | `Security_Scripts/` | Findings from tools such as `port-scanner`, `vuln_check`, `password_spray`, and `shodan-scanner` | `security-scans-*` |
| Custom IDS and detection engineering | `custom-ids/` | Alerts created by the curated Suricata/Snort rulesets | `ids-alerts-*` |
| Secure configuration automation | `ansible-hardening/` | Compliance status per control and host | `ansible-compliance-*` |
| Tamper evident ledgering | `blockchain-secure-logging/` | Blockchain anchoring state, off-chain notarization metadata | `blockchain-audit-*` |
| Mobile application analysis | `mobile-security-analysis/` | Static and dynamic risk scoring for APKs | `mobile-findings-*` |
| Quantum/RNG experimentation | `quantum-computing/` | QRNG health, Kyber posture, and other emerging-tech experiments | `quantum-research-*` |

The upgraded stack combines Elasticsearch, Logstash, Kibana, and Grafana.  Logstash normalizes JSON artifacts produced by each toolset, Elasticsearch stores each domain inside its own time-series index, Kibana ships opinionated dashboards for SOC workflows, and Grafana provides SRE-friendly health boards.  Everything runs locally through Docker Compose so that the blueprint can be hardened and scaled within your preferred orchestrator afterwards.

## Stack components

| Service | Purpose |
|---------|---------|
| **Elasticsearch** | Schema-flexible datastore that backs the dashboard. Security monitoring indices are prefixed per capability so lifecycle and security policies can be tailored. |
| **Logstash** | File-based ingest pipeline that reads JSON Lines events from `data/ingest/`, enriches them with metadata, and publishes to the appropriate index. Beats, Kafka, or HTTP inputs can be enabled by extending `logstash/pipeline/logstash.conf`. |
| **Kibana** | Analyst UX. The blueprint includes index-pattern definitions, visualization descriptions, and a multi-panel dashboard skeleton in `kibana/`. Import them via Stack Management → Saved Objects. |
| **Grafana** | Provides a parallel, ops-friendly view that can mix Elasticsearch and external metrics. Datasource provisioning lives in `grafana/provisioning/`. |

## Repository-aware data model
Each major tool in the portfolio emits JSON documents with a `target_index` field.  Logstash reads the field and writes to an index of the same name.  The sample dataset stored in `data/ingest/enterprise-events.jsonl` demonstrates the mapping and references real artifacts from across the repo (for example `custom-ids/rules/enterprise.rules` and `Security_Scripts/vuln_check`).

The helper script `scripts/build_enterprise_dataset.py` regenerates the JSON Lines file and injects live metadata (for example, the number of IDS rule packs or the list of reconnaissance scripts that are present on disk).  Treat the script as the canonical contract for field names when wiring in additional tools.

## Quick start
1. **Generate / refresh sample data**
   ```bash
   python3 scripts/build_enterprise_dataset.py
   ```
   The script inspects the repository tree and rewrites `data/ingest/enterprise-events.jsonl` with synthetic yet realistic events for every capability in the table above.
2. **Launch the stack**
   ```bash
   docker-compose up -d
   ```
   The compose file now starts Elasticsearch, Logstash, Kibana, and Grafana.  Logstash immediately begins reading the JSON Lines file and publishing documents into the six security indices plus the `platform-inventory` summary index.
3. **Import the dashboard blueprints**
   * In Kibana, open *Stack Management → Saved Objects → Import* and feed it the files under `kibana/`.  They describe index patterns, visualization goals, and a SOC dashboard layout tailored to this repository.
   * In Grafana, log in with the default admin credentials (`admin` / `changeme`, change on first login).  The Elasticsearch datasource is provisioned automatically; add panels that query the indices defined above or reuse the JSON from `grafana/dashboards/`.
4. **Point real tools at the stack**
   * Drop JSON Lines outputs from scripts inside `Security_Scripts/` or reports emitted by `mobile-security-analysis/analyze_apk.py` into `data/ingest/` and rerun Logstash (or send them via Filebeat/HTTP).
   * Forward Suricata/Snort EVE JSON from `custom-ids/` sensors to the Beats port (`5044`) exposed by Logstash.
   * Stream compliance summaries generated by `ansible-hardening/` playbooks or blockchain receipts from `blockchain-secure-logging/` to the same ingest path.

## Extending the blueprint
- **Index lifecycle and security** – In production enable TLS and role-based access control, carve data streams per capability, and attach ILM policies based on retention requirements.
- **Automation** – Wire the scripts in `Security_Scripts/` and the playbooks under `ansible-hardening/` to CI/CD so that every run ships JSON to this dashboard automatically.
- **Alerting** – Enable Kibana alerting or Grafana OnCall using the provided visualizations as starting points.  For example, build threshold alerts on `ids-alerts-*` severity counts.
- **Ledger verification** – Use `blockchain-secure-logging/offchain/` services to verify hashes displayed on the blockchain audit panel before closing incidents.

## File map
```
security-dashboard/
├── data/ingest/enterprise-events.jsonl    # Repository-aware sample events
├── docker-compose.yml                    # Multi-service stack definition
├── logstash/pipeline/logstash.conf       # Metadata-aware ingest pipeline
├── grafana/provisioning/                 # Automatic Grafana datasource config
├── kibana/                               # Index patterns, visualization specs, and dashboard layout
├── scripts/build_enterprise_dataset.py   # Regenerate the JSON Lines dataset
└── scripts/architecture.md               # Detailed description of the enterprise topology
```
- In Kibana, create a data view on security-scans and set timestamp as the time field.
- Build visualizations and dashboards based on your scan data.

## Notes
- Use .keyword fields for aggregation in visualizations (e.g., vulnerability.keyword).
- Refresh the field list in Kibana if new fields aren’t showing.
- This is a base setup; further ingestion automation and dashboards will be developed.

## Custom Security Dashboard

### Scope
Real-time visualization of security events and metrics with a path toward enterprise-scale dashboards.

### Tools
- Grafana
- Kibana
- Custom data sources

### Implementation
1. Install visualization tools.
2. Connect to log and data streams.
3. Build visualizations and alerting.

### Challenges
- Data parsing
- Real-time ingestion
- Uptime

### Next Steps
- Automate data feeds ✅
- Configure alerts ✅
- Plan for enterprise-level scaling ✅
- Add Logstash and Beats pipelines for structured ingest ✅
- Enable TLS and role-based access control ✅
- Implement index lifecycle management and snapshots ✅

## Automated Data Feeds
Use the included Python helper to generate normalized events that Filebeat can forward to Logstash.

```bash
python3 scripts/automate_feeds.py \\
  --batch 25 --interval 3 --max-cycles 5 \\
  --stdout --seed 42 \\
  --logstash-endpoint https://localhost:9601
```

- The script appends newline-delimited JSON to `data/feeds/security-events.log` and can mirror events to stdout with `--stdout`.
- Use `--max-cycles` for deterministic CI runs and `--seed` to reproduce specific datasets.
- Filebeat (configured in `beats/filebeat.yml`) tails the file and sends events over TLS to Logstash.
- If the optional `--logstash-endpoint` is set, events are also pushed to the HTTPS Logstash input for SOAR integrations.

## Logstash and Beats Pipelines
- `logstash/pipeline/security.conf` normalizes timestamps, adds metadata, and writes into `security-scans-*` indices.
- `logstash/config/logstash.yml` enables persistent queues plus TLS-secured monitoring against Elasticsearch.
- `beats/filebeat.yml` defines a filestream input and disables ILM so Logstash/Elasticsearch manage lifecycle policies.
- Update Filebeat fields or add new inputs for additional scanners while keeping the same Logstash output stanza.

## TLS and Role-Based Access Control
TLS is now required between every component. Generate certificates before starting Docker Compose by following `certs/README.md`.

1. Create `.env` with stack credentials:

```
ELASTIC_PASSWORD=changeme
ELASTIC_USER=elastic
LOGSTASH_USER=logstash_writer
LOGSTASH_PASSWORD=logstash_writer_password
```

2. Bootstrap users and roles once Elasticsearch is up:

```bash
curl -u elastic:$ELASTIC_PASSWORD --cacert certs/ca.crt \\
  -X POST https://localhost:9200/_security/role/logstash_writer -H 'Content-Type: application/json' -d '{
    "cluster": ["monitor", "manage_index_templates", "manage_ilm"],
    "indices": [{"names": ["security-scans-*"], "privileges": ["write", "create_index"]}]
  }'

curl -u elastic:$ELASTIC_PASSWORD --cacert certs/ca.crt \\
  -X POST https://localhost:9200/_security/user/logstash_writer -H 'Content-Type: application/json' -d '{
    "password": "'$LOGSTASH_PASSWORD'",
    "roles": ["logstash_writer"],
    "full_name": "Logstash Output"
  }'
```

3. Kibana enforces TLS for the UI (`https://localhost:5601`). Use the built-in `elastic` superuser for bootstrap, then create analyst roles scoped to required data views.

## Alerting
- Import `scripts/alerts/critical-vulnerability-watcher.json` in **Stack Management → Watcher** to log when critical findings arrive.
- Kibana Security Solution detection rules can also be created to send email/Webhook/PagerDuty notifications.
- Because Logstash exposes an HTTPS HTTP input on port `9601`, external SOAR tooling can push enrichment data using mutual TLS.

## Index Lifecycle Management & Snapshots
`scripts/ilm_and_snapshots.sh` creates an ILM policy, index template, snapshot repository, and an initial snapshot.

```bash
chmod +x scripts/ilm_and_snapshots.sh
ES_PASS=$ELASTIC_PASSWORD ./scripts/ilm_and_snapshots.sh \
  ES_URL=https://localhost:9200 ES_USER=elastic CACERT=certs/ca.crt
```

- ILM stages follow a hot/warm/cold/delete progression (7/30/90 days by default).
- Snapshots write to the `snapshots` Docker volume (`/usr/share/elasticsearch/snapshots`). Point this to an object store in production.

## Enterprise-Level Scaling
See `scripts/architecture.md` for a detailed roadmap that covers ingest horizontal scaling, Kafka buffering, RBAC, observability, and DR patterns.

### Professional ELK Stack Comparison

| Feature | Base Setup | Professional Deployment |
|--------|------------|-------------------------|
| Topology | Single-node Elasticsearch and Kibana | Multi-node clusters with load balancers |
| Ingestion | Manual curl or custom scripts | Beats agents feeding Logstash pipelines |
| Security | xpack security disabled | TLS encryption and role-based access control |
| Management | Ad-hoc indices | Index lifecycle management and snapshot backups |
| Monitoring | Manual checks | Centralized monitoring and alerting with X-Pack |

### Resources
- [Grafana Docs](https://grafana.com/docs/)

## License & disclaimer
This project is for educational and authorized professional use only. Do not use on unauthorized networks or systems.  Always obtain permission before running active security tooling.
